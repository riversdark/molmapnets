{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af3602",
   "metadata": {},
   "source": [
    "# bace\n",
    "> Using molmapnets for classification, with descriptors, or fingerprints, or both. Tested on the [BACE](https://drugdesigndata.org/about/grand-challenge-4/bace) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0131a410-c2c6-424c-a1f4-6a2935436eb6",
   "metadata": {},
   "source": [
    "Per it's own documentation:\n",
    "\n",
    "> Beta-Secretase 1 (BACE) is a transmembrane aspartic-acid protease human protein encoded by the BACE1 gene. BACE is essential for the generation of beta-amyloid peptide in neural tissue1, a component of amyloid plaques widely believed to be critical in the development of Alzheimer's, rendering BACE an attractive therapeutic target for this devastating disease2.\n",
    "\n",
    "> The BACE dataset provided herein comprises small molecule inhibitors across a three order of magnitude (nM to Î¼M) range of IC50s along with previously undisclosed crystallographic structures. Specifically, we provide 154 BACE inhibitors for affinity, 20 for pose, and 34 for free energy prediction. This dataset was kindly provided by Novartis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08473ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b581d8-3fb3-4863-87f4-c7a582278706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [10:25:36] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from chembench import dataset\n",
    "from molmap import MolMap, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0ec11-c6a8-41c5-9383-e2404e48aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from molmapnets.data import SingleFeatureData, DoubleFeatureData\n",
    "from molmapnets.models import MolMapMultiClassClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291a8b2",
   "metadata": {},
   "source": [
    "## Feature extraction \n",
    "\n",
    "The `chembench` package collected several different datasets for benchmarking the models. Here we'll use the [`eSOL`](http://www.tanpaku.org/tp-esol/index.php?lang=en) dataset, which collects the solubility of all E.coli proteins. The data can be loaded with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c669416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 1513\n"
     ]
    }
   ],
   "source": [
    "data = dataset.load_BACE()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1e4b4-fba9-4b8a-bfd1-dc6c98abc4e5",
   "metadata": {},
   "source": [
    "Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c5023-70c8-4cc0-bd5f-930fa681af09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>O1CC[C@@H](NC(=O)[C@@H](Cc2cc3cc(ccc3nc2N)-c2c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Fc1cc(cc(F)c1)C[C@H](NC(=O)[C@@H](N1CC[C@](NC(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>S1(=O)(=O)N(c2cc(cc3c2n(cc3CC)CC1)C(=O)N[C@H](...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>S1(=O)(=O)C[C@@H](Cc2cc(O[C@H](COCC)C(F)(F)F)c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>S1(=O)(=O)N(c2cc(cc3c2n(cc3CC)CC1)C(=O)N[C@H](...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  Class\n",
       "0  O1CC[C@@H](NC(=O)[C@@H](Cc2cc3cc(ccc3nc2N)-c2c...      1\n",
       "1  Fc1cc(cc(F)c1)C[C@H](NC(=O)[C@@H](N1CC[C@](NC(...      1\n",
       "2  S1(=O)(=O)N(c2cc(cc3c2n(cc3CC)CC1)C(=O)N[C@H](...      1\n",
       "3  S1(=O)(=O)C[C@@H](Cc2cc(O[C@H](COCC)C(F)(F)F)c...      1\n",
       "4  S1(=O)(=O)N(c2cc(cc3c2n(cc3CC)CC1)C(=O)N[C@H](...      1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad9cfe-d67f-45be-bba0-b09d92cd0321",
   "metadata": {},
   "source": [
    "This is a two class classification data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4cedb-6f3e-4ee8-81a0-96b1d5070d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df.Class.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0cbc40",
   "metadata": {},
   "source": [
    "Create feature map objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad885be-d0eb-42fa-9e58-3650f07dfd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PubChemFP0', 'PubChemFP1', 'PubChemFP2', 'PubChemFP3', 'PubChemFP4']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitsinfo = feature.fingerprint.Extraction().bitsinfo\n",
    "flist = bitsinfo[bitsinfo.Subtypes.isin(['PubChemFP'])].IDs.tolist()\n",
    "\n",
    "flist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651a8fab-4e4a-46f7-88be-1e8a79e360b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor = MolMap(ftype='descriptor', metric='cosine',)\n",
    "fingerprint = MolMap(ftype='fingerprint', fmap_type='scatter', flist=flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776cdbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-23 10:25:51,529 - INFO - [bidd-molmap] - Applying grid feature map(assignment), this may take several minutes(1~30 min)\n",
      "2021-07-23 10:25:54,623 - INFO - [bidd-molmap] - Finished\n"
     ]
    }
   ],
   "source": [
    "descriptor.fit(verbose=0, method='umap', min_dist=0.1, n_neighbors=15,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20423af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-23 10:25:55,712 - INFO - [bidd-molmap] - Applying naive scatter feature map...\n",
      "2021-07-23 10:25:55,732 - INFO - [bidd-molmap] - Finished\n"
     ]
    }
   ],
   "source": [
    "fingerprint.fit(verbose=0, method='umap', min_dist=0.1, n_neighbors=15,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0bffa-cff3-4c5e-8ffd-d9c61c6fa16a",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb99e4-a3d9-4b02-a38d-e55a7e9ff18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1513/1513 [09:00<00:00,  3.06it/s]\n",
      "100%|##########| 1513/1513 [02:13<00:00, 11.30it/s]\n"
     ]
    }
   ],
   "source": [
    "X1 = descriptor.batch_transform(data.x)\n",
    "X2 = fingerprint.batch_transform(data.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1cc63b-344f-4611-b2bb-bba90b0fc326",
   "metadata": {},
   "source": [
    "We also need to transform the outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87164e0-66e1-4758-b393-43acbb730f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1513, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.get_dummies(data.df['Class']).values\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f399d54",
   "metadata": {},
   "source": [
    "We can visualise the feature maps easily with MolMap, but the visualisations are removed to avoid crushing the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347fca4",
   "metadata": {},
   "source": [
    "## Classification using only the descriptor map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f60340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_feature = SingleFeatureData(Y, X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = random_split(single_feature, [1113, 200, 200], generator=torch.Generator().manual_seed(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79962896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1113, 200, 200)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae870cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c26f3",
   "metadata": {},
   "source": [
    "And we can get one batch of data by making the data loader iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae73cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4229a2-2e8a-45b3-848d-d777de4306d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8),\n",
       "indices=tensor([0, 1, 1, 0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(t, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af725f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 13, 37, 37])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173afeb6",
   "metadata": {},
   "source": [
    "Finally with the data prepared we can train the models. These are tests to show that the models work as expected, but we can certainly fine tune the model to achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbce54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MolMapMultiClassClassification(n_class=2)\n",
    "\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ea140-2765-4d50-b939-869ca049cba6",
   "metadata": {},
   "source": [
    "Predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd3931-6087-4bce-bb0e-42633270a16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6294, 0.3706],\n",
       "        [0.4291, 0.5709],\n",
       "        [0.3947, 0.6053],\n",
       "        [0.9203, 0.0797],\n",
       "        [0.8640, 0.1360],\n",
       "        [0.9787, 0.0213],\n",
       "        [0.6260, 0.3740],\n",
       "        [0.4413, 0.5587]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81777976-bb5a-4153-9ea4-8bdbbf90a2c8",
   "metadata": {},
   "source": [
    "And we can get the predicted class by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791d521-7678-4a3b-866f-6e8ef66bd6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(model(x).exp(), 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88bf3a",
   "metadata": {},
   "source": [
    "And the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9e3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Iter:    50] Training loss: 0.599\n",
      "[Epoch: 1, Iter:   100] Training loss: 0.600\n",
      "[Epoch: 2, Iter:    50] Training loss: 0.591\n",
      "[Epoch: 2, Iter:   100] Training loss: 0.591\n",
      "[Epoch: 3, Iter:    50] Training loss: 0.580\n",
      "[Epoch: 3, Iter:   100] Training loss: 0.589\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (xb, yb) in enumerate(train_loader):\n",
    "        \n",
    "        # fix output shape for loss function        \n",
    "        yb = torch.max(yb, 1)[1]\n",
    "        \n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward propagation\n",
    "        pred = model(xb)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 50 == 0:    \n",
    "            print('[Epoch: %d, Iter: %5d] Training loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / (i+1)))\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab507bf8",
   "metadata": {},
   "source": [
    "And let's look at the prediction accuracy on validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 68 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (xb, yb) in enumerate(val_loader):\n",
    "\n",
    "        yb = torch.max(yb, 1)[1]\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        pred = model(xb)\n",
    "        pred = torch.max(pred, 1)[1]\n",
    "\n",
    "        # accuracy calculation\n",
    "        total += yb.size(0)\n",
    "        correct += (pred==yb).sum().item()\n",
    "\n",
    "        \n",
    "print('Accuracy of the network on the test data: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8c981-75b4-4b0a-8bb6-f3a7ea86c58c",
   "metadata": {},
   "source": [
    "Hmm not very good, but again we only trained for 5 epochs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c478288-e476-4280-83e4-2a8797a6647b",
   "metadata": {},
   "source": [
    "## Classificatin using both feature maps\n",
    "\n",
    "\n",
    "Now we can feed both the feature maps to the model as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_feature = DoubleFeatureData(Y, (X1, X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_double, val_double, test_double = random_split(double_feature, [1113, 200, 200], generator=torch.Generator().manual_seed(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a52a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1113, 200, 200)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_double), len(val_double), len(test_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_double = DataLoader(train_double, batch_size=8, shuffle=True)\n",
    "val_loader_double = DataLoader(val_double, batch_size=8, shuffle=True)\n",
    "test_loader_double = DataLoader(test_double, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b3ba1",
   "metadata": {},
   "source": [
    "And we can get one batch of data by making the data loader iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = next(iter(train_loader_double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd427cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88730ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 13, 37, 37]), torch.Size([8, 1, 52, 52]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2 = x\n",
    "x1.shape, x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d407b9",
   "metadata": {},
   "source": [
    "And classification. Different feature maps have different number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_double = MolMapMultiClassClassification(conv_in1=13, conv_in2=1)\n",
    "\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_double.to(device)\n",
    "optimizer = optim.Adam(model_double.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af565cc7",
   "metadata": {},
   "source": [
    "And the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Iter:    50] Training loss: 1.000\n",
      "[Epoch: 1, Iter:   100] Training loss: 0.864\n",
      "[Epoch: 2, Iter:    50] Training loss: 0.709\n",
      "[Epoch: 2, Iter:   100] Training loss: 0.706\n",
      "[Epoch: 3, Iter:    50] Training loss: 0.696\n",
      "[Epoch: 3, Iter:   100] Training loss: 0.708\n",
      "[Epoch: 4, Iter:    50] Training loss: 0.698\n",
      "[Epoch: 4, Iter:   100] Training loss: 0.694\n",
      "[Epoch: 5, Iter:    50] Training loss: 0.628\n",
      "[Epoch: 5, Iter:   100] Training loss: 0.616\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, ((x1, x2), yb) in enumerate(train_loader_double):\n",
    "\n",
    "        # fix output shape for loss function        \n",
    "        yb = torch.max(yb, 1)[1]\n",
    "\n",
    "        x1, x2, yb = x1.to(device), x2.to(device), yb.to(device)\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward propagation\n",
    "        pred = model_double((x1, x2))\n",
    "\n",
    "        # loss calculation\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 50 == 0:    \n",
    "            print('[Epoch: %d, Iter: %5d] Training loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / (i+1)))\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f58be",
   "metadata": {},
   "source": [
    "Loss on validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7cd40-ea1d-499a-812f-e82d5958edf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data: 75 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, ((x1, x2), yb) in enumerate(val_loader_double):\n",
    "\n",
    "        yb = torch.max(yb, 1)[1]\n",
    "        x1, x2, yb = x1.to(device), x2.to(device), yb.to(device)\n",
    "\n",
    "        pred = model_double((x1, x2))\n",
    "        pred = torch.max(pred, 1)[1]\n",
    "\n",
    "        # accuracy calculation\n",
    "        total += yb.size(0)\n",
    "        correct += (pred==yb).sum().item()\n",
    "\n",
    "        \n",
    "print('Accuracy of the network on the test data: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5a314-4ce2-49e9-9a92-36bd38e9f646",
   "metadata": {},
   "source": [
    "Nice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
